{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# The modules we're going to use\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, convolutional, pooling, Flatten, Dropout\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before pre-processing, X_train size:  (11322, 2304)\n",
      "Before pre-processing, y_train size:  (11322, 1)\n",
      "Before pre-processing, X_test size:  (3965, 2304)\n",
      "Before pre-processing, X_val size:  (4853, 2304)\n",
      "Before pre-processing, y_val size:  (4853, 1)\n",
      "After pre-processing, X_train size:  (11322, 1, 48, 48)\n",
      "After pre-processing, y_train size:  (11322, 3)\n",
      "After pre-processing, X_test size:  (3965, 1, 48, 48)\n",
      "After pre-processing, X_val size:  (4853, 1, 48, 48)\n",
      "After pre-processing, y_val size:  (4853, 3)\n"
     ]
    }
   ],
   "source": [
    "# load the training and target data from the data set (which is csv)\n",
    "train_data = np.genfromtxt('./train_data.csv',delimiter=',')\n",
    "train_target = np.genfromtxt('./train_target.csv', delimiter=',')\n",
    "# load the test data also\n",
    "test_data = np.genfromtxt('./test_data.csv',delimiter=',')\n",
    "# here we split the training data to get some data for cross validation\n",
    "train_data, val_data, train_target, val_target = train_test_split(train_data, (train_target[:, np.newaxis]), test_size=0.3, random_state=42)\n",
    "# display dataset shapes before processing\n",
    "print('Before pre-processing, X_train size: ', train_data.shape)\n",
    "print('Before pre-processing, y_train size: ', train_target.shape)\n",
    "print('Before pre-processing, X_test size: ', test_data.shape)\n",
    "print('Before pre-processing, X_val size: ', val_data.shape)\n",
    "print('Before pre-processing, y_val size: ', val_target.shape)\n",
    "# reshape the data to match the pixles of the image (-1, = stays same, 1 = 1 channel, 48 & 48means 48x48 image)\n",
    "train_data = train_data.reshape(-1,1, 48,48)\n",
    "test_data = test_data.reshape(-1,1, 48,48)\n",
    "val_data = val_data.reshape(-1,1, 48,48)\n",
    "# break down the targets into forms of 0,1, or 2\n",
    "train_target = np_utils.to_categorical(train_target, 3)\n",
    "val_target = np_utils.to_categorical(val_target, 3)\n",
    "# display the new shapes\n",
    "print('After pre-processing, X_train size: ', train_data.shape)\n",
    "print('After pre-processing, y_train size: ', train_target.shape)\n",
    "print('After pre-processing, X_test size: ', test_data.shape)\n",
    "print('After pre-processing, X_val size: ', val_data.shape)\n",
    "print('After pre-processing, y_val size: ', val_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a new model\n",
    "model = Sequential()\n",
    "# create a convolutional layer for 2 dimensions\n",
    "# this one includes the input size ofr first layer\n",
    "model.add(convolutional.Conv2D(\n",
    "    filters=32,\n",
    "    kernel_size=(2, 2),\n",
    "    padding='same',\n",
    "    strides=(1, 1),\n",
    "    input_shape=train_data.shape[1:],\n",
    "    activation='relu'\n",
    "))\n",
    "# create a convolutional layer for 2 dimensions\n",
    "model.add(convolutional.Conv2D(\n",
    "    filters=32,\n",
    "    kernel_size=(2, 2),\n",
    "    padding='same',\n",
    "    strides=(1, 1),\n",
    "    activation='relu'\n",
    "))\n",
    "# create a convolutional layer for 2 dimensions\n",
    "model.add(convolutional.Conv2D(\n",
    "    filters=32,\n",
    "    kernel_size=(2, 2),\n",
    "    padding='same',\n",
    "    strides=(1, 1),\n",
    "    activation='relu'\n",
    "))\n",
    "\n",
    "model.add(Dropout(.1))\n",
    "\n",
    "# create a max pooling layer for 2 dimensions\n",
    "model.add(pooling.MaxPooling2D(\n",
    "    pool_size=(2, 2),\n",
    "    padding='same',\n",
    "))\n",
    "# create a convolutional layer for 2 dimensions\n",
    "model.add(convolutional.Conv2D(\n",
    "    filters=64,\n",
    "    kernel_size=(2, 2),\n",
    "    padding='same',\n",
    "    strides=(1, 1),\n",
    "    activation='relu'\n",
    "))\n",
    "# create a convolutional layer for 2 dimensions\n",
    "model.add(convolutional.Conv2D(\n",
    "    filters=64,\n",
    "    kernel_size=(2, 2),\n",
    "    padding='same',\n",
    "    strides=(1, 1),\n",
    "    activation='relu'\n",
    "))\n",
    "# create a convolutional layer for 2 dimensions\n",
    "model.add(convolutional.Conv2D(\n",
    "    filters=64,\n",
    "    kernel_size=(2, 2),\n",
    "    padding='same',\n",
    "    strides=(1, 1),\n",
    "    activation='relu'\n",
    "))\n",
    "\n",
    "model.add(Dropout(.1))\n",
    "\n",
    "# create a max pooling layer for 2 dimensions\n",
    "model.add(pooling.MaxPooling2D(\n",
    "    pool_size=(2, 2),\n",
    "    padding='same',\n",
    "))\n",
    "# create a convolutional layer for 2 dimensions\n",
    "model.add(convolutional.Conv2D(\n",
    "    filters=128,\n",
    "    kernel_size=(2, 2),\n",
    "    padding='same',\n",
    "    strides=(1, 1),\n",
    "    activation='relu'\n",
    "))\n",
    "# create a convolutional layer for 2 dimensions\n",
    "model.add(convolutional.Conv2D(\n",
    "    filters=128,\n",
    "    kernel_size=(2, 2),\n",
    "    padding='same',\n",
    "    strides=(1, 1),\n",
    "    activation='relu'\n",
    "))\n",
    "# create a convolutional layer for 2 dimensions\n",
    "model.add(convolutional.Conv2D(\n",
    "    filters=128,\n",
    "    kernel_size=(2, 2),\n",
    "    padding='same',\n",
    "    strides=(1, 1),\n",
    "    activation='relu'\n",
    "))\n",
    "\n",
    "model.add(Dropout(.1))\n",
    "\n",
    "\n",
    "# create a max pooling layer for 2 dimensions\n",
    "model.add(pooling.MaxPooling2D(\n",
    "    pool_size=(2, 2),\n",
    "    padding='same',\n",
    "))\n",
    "# flatten the model's layer into a deep neuron which in turn, will be part of the fully connected feedforward network\n",
    "model.add(Flatten())\n",
    "# create dorpout layer to help with overfitting\n",
    "model.add(Dropout(.5))\n",
    "# create a single deep layer with a depth of 1024 for the output space\n",
    "model.add(Dense(1024))\n",
    "# choosing this as the activation type\n",
    "model.add(Activation('relu'))\n",
    "# create dorpout layer to help with overfitting\n",
    "model.add(Dropout(.2))\n",
    "# last layer is the softmax layer to get the probability of each class\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11322 samples, validate on 4853 samples\n",
      "Epoch 1/50\n",
      "12s - loss: 1.0653 - acc: 0.4401 - val_loss: 1.0453 - val_acc: 0.4587\n",
      "Epoch 2/50\n",
      "12s - loss: 1.0298 - acc: 0.4732 - val_loss: 0.9879 - val_acc: 0.5184\n",
      "Epoch 3/50\n",
      "13s - loss: 0.9737 - acc: 0.5329 - val_loss: 0.9416 - val_acc: 0.5568\n",
      "Epoch 4/50\n",
      "15s - loss: 0.9381 - acc: 0.5552 - val_loss: 0.9022 - val_acc: 0.5932\n",
      "Epoch 5/50\n",
      "13s - loss: 0.8966 - acc: 0.5858 - val_loss: 0.9073 - val_acc: 0.5803\n",
      "Epoch 6/50\n",
      "13s - loss: 0.8810 - acc: 0.5926 - val_loss: 0.8807 - val_acc: 0.5980\n",
      "Epoch 7/50\n",
      "13s - loss: 0.8575 - acc: 0.6094 - val_loss: 0.8807 - val_acc: 0.5998\n",
      "Epoch 8/50\n",
      "14s - loss: 0.8442 - acc: 0.6104 - val_loss: 0.8795 - val_acc: 0.6011\n",
      "Epoch 9/50\n",
      "14s - loss: 0.8325 - acc: 0.6144 - val_loss: 0.8785 - val_acc: 0.5998\n",
      "Epoch 10/50\n",
      "20s - loss: 0.8161 - acc: 0.6289 - val_loss: 0.8780 - val_acc: 0.5978\n",
      "Epoch 11/50\n",
      "21s - loss: 0.8076 - acc: 0.6357 - val_loss: 0.8543 - val_acc: 0.6122\n",
      "Epoch 12/50\n",
      "15s - loss: 0.7971 - acc: 0.6363 - val_loss: 0.8387 - val_acc: 0.6163\n",
      "Epoch 13/50\n",
      "13s - loss: 0.7810 - acc: 0.6441 - val_loss: 0.8589 - val_acc: 0.6103\n",
      "Epoch 14/50\n",
      "13s - loss: 0.7730 - acc: 0.6534 - val_loss: 0.8393 - val_acc: 0.6184\n",
      "Epoch 15/50\n",
      "15s - loss: 0.7653 - acc: 0.6532 - val_loss: 0.8481 - val_acc: 0.6211\n",
      "Epoch 16/50\n",
      "13s - loss: 0.7606 - acc: 0.6566 - val_loss: 0.8223 - val_acc: 0.6274\n",
      "Epoch 17/50\n",
      "13s - loss: 0.7436 - acc: 0.6719 - val_loss: 0.8309 - val_acc: 0.6272\n",
      "Epoch 18/50\n",
      "13s - loss: 0.7342 - acc: 0.6744 - val_loss: 0.8193 - val_acc: 0.6340\n",
      "Epoch 19/50\n",
      "14s - loss: 0.7148 - acc: 0.6839 - val_loss: 0.8246 - val_acc: 0.6266\n",
      "Epoch 20/50\n",
      "15s - loss: 0.7107 - acc: 0.6849 - val_loss: 0.8194 - val_acc: 0.6347\n",
      "Epoch 21/50\n",
      "24s - loss: 0.7032 - acc: 0.6937 - val_loss: 0.8460 - val_acc: 0.6270\n",
      "Epoch 22/50\n",
      "24s - loss: 0.6810 - acc: 0.6997 - val_loss: 0.8496 - val_acc: 0.6215\n",
      "Epoch 23/50\n",
      "25s - loss: 0.6712 - acc: 0.7086 - val_loss: 0.8956 - val_acc: 0.6126\n",
      "Epoch 24/50\n",
      "27s - loss: 0.6600 - acc: 0.7152 - val_loss: 0.8505 - val_acc: 0.6277\n",
      "Epoch 25/50\n",
      "29s - loss: 0.6617 - acc: 0.7110 - val_loss: 0.8211 - val_acc: 0.6357\n",
      "Epoch 26/50\n",
      "26s - loss: 0.6346 - acc: 0.7250 - val_loss: 0.8488 - val_acc: 0.6242\n",
      "Epoch 27/50\n",
      "25s - loss: 0.6292 - acc: 0.7289 - val_loss: 0.8720 - val_acc: 0.6324\n",
      "Epoch 28/50\n",
      "25s - loss: 0.6147 - acc: 0.7334 - val_loss: 0.8539 - val_acc: 0.6324\n",
      "Epoch 29/50\n",
      "25s - loss: 0.6009 - acc: 0.7376 - val_loss: 0.8448 - val_acc: 0.6427\n",
      "Epoch 30/50\n",
      "26s - loss: 0.5775 - acc: 0.7555 - val_loss: 0.8367 - val_acc: 0.6353\n",
      "Epoch 31/50\n",
      "26s - loss: 0.5702 - acc: 0.7542 - val_loss: 0.8500 - val_acc: 0.6410\n",
      "Epoch 32/50\n",
      "25s - loss: 0.5586 - acc: 0.7569 - val_loss: 0.9148 - val_acc: 0.6326\n",
      "Epoch 33/50\n",
      "25s - loss: 0.5485 - acc: 0.7703 - val_loss: 0.9138 - val_acc: 0.6274\n",
      "Epoch 34/50\n",
      "26s - loss: 0.5465 - acc: 0.7675 - val_loss: 0.8491 - val_acc: 0.6478\n",
      "Epoch 35/50\n",
      "26s - loss: 0.5234 - acc: 0.7812 - val_loss: 0.9002 - val_acc: 0.6373\n",
      "Epoch 36/50\n",
      "25s - loss: 0.5014 - acc: 0.7886 - val_loss: 0.8925 - val_acc: 0.6301\n",
      "Epoch 37/50\n",
      "25s - loss: 0.4961 - acc: 0.7895 - val_loss: 0.9302 - val_acc: 0.6431\n",
      "Epoch 38/50\n",
      "25s - loss: 0.4787 - acc: 0.7990 - val_loss: 0.9264 - val_acc: 0.6398\n",
      "Epoch 39/50\n",
      "25s - loss: 0.4614 - acc: 0.8090 - val_loss: 0.9874 - val_acc: 0.6342\n",
      "Epoch 40/50\n",
      "25s - loss: 0.4626 - acc: 0.8082 - val_loss: 0.9744 - val_acc: 0.6330\n",
      "Epoch 41/50\n",
      "25s - loss: 0.4584 - acc: 0.8111 - val_loss: 1.0450 - val_acc: 0.6427\n",
      "Epoch 42/50\n",
      "25s - loss: 0.4133 - acc: 0.8274 - val_loss: 1.0200 - val_acc: 0.6425\n",
      "Epoch 43/50\n",
      "21s - loss: 0.4227 - acc: 0.8234 - val_loss: 1.0546 - val_acc: 0.6349\n",
      "Epoch 44/50\n",
      "14s - loss: 0.4112 - acc: 0.8270 - val_loss: 1.0477 - val_acc: 0.6332\n",
      "Epoch 45/50\n",
      "14s - loss: 0.3885 - acc: 0.8387 - val_loss: 0.9858 - val_acc: 0.6235\n",
      "Epoch 46/50\n",
      "14s - loss: 0.4177 - acc: 0.8290 - val_loss: 1.0389 - val_acc: 0.6303\n",
      "Epoch 47/50\n",
      "13s - loss: 0.3760 - acc: 0.8476 - val_loss: 1.0829 - val_acc: 0.6301\n",
      "Epoch 48/50\n",
      "14s - loss: 0.3608 - acc: 0.8546 - val_loss: 1.1167 - val_acc: 0.6198\n",
      "Epoch 49/50\n",
      "16s - loss: 0.3763 - acc: 0.8485 - val_loss: 1.0508 - val_acc: 0.6217\n",
      "Epoch 50/50\n",
      "22s - loss: 0.3728 - acc: 0.8463 - val_loss: 1.1220 - val_acc: 0.6246\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# choose adam as the optimizer\n",
    "# the optmizer chooses the adaptive learnign rates which are used for the Stochastic gradient descent\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "# compile (configure) the model's learning process\n",
    "model.compile(\n",
    "    # choose adam as optimizer\n",
    "    optimizer = adam,\n",
    "    # uses cross entroyphy for loss function\n",
    "    loss='categorical_crossentropy',\n",
    "    # A metric function is similar to an loss function, except that the results from evaluating a metric are not used when training the model.\n",
    "    # https://keras.io/metrics/\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "# fit the model with the data to train ig, and data to vlaidate it,\n",
    "# also define the epochs and the batch size for each epochs\n",
    "model.fit(train_data, train_target, validation_data=(val_data, val_target), epochs=50, batch_size=128, verbose=2)\n",
    "# model.fit(train_data, train_target, validation_split=0.3, validation_data=val_data, epochs=50, batch_size=128, verbose=2)\n",
    "# get loss amount and accuracy of the validation set\n",
    "loss, accuracy = model.evaluate(val_data, val_target, verbose=2)\n",
    "print('test loss:', loss)\n",
    "print('test accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Show the image of one testing example\n",
    "# get random number within range of the test data\n",
    "temp = np.random.randint(test_data.shape[0], size=1)\n",
    "# Get its prediction\n",
    "output = model.predict(test_data[temp[0]].reshape(-1,1, 48, 48))\n",
    "# output prediction\n",
    "print(output)\n",
    "# create new figure using matplotlib\n",
    "plt.figure()\n",
    "# display and plot image\n",
    "plt.xticks(np.arange(output.shape[1]))\n",
    "plt.plot(np.arange(output.shape[1]), output.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# get predictions\n",
    "predictions = model.predict(test_data)\n",
    "# The maximum value along a given axis.\n",
    "# https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.argmax.html\n",
    "output = predictions.argmax(axis=1)\n",
    "# open new file of my choosing\n",
    "f = open('./tanner_summers_hw2_results.csv','w')\n",
    "# Write category header\n",
    "f.write('Id,Category\\n')\n",
    "# loop data and print to file the iteration and it's target for that iteration at i\n",
    "for i in range(0, test_data.shape[0]):\n",
    "    # write data to file\n",
    "    f.write(str(i) + ',' + str(output[i]) + '\\n')\n",
    "# close file\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
