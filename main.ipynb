{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# The modules we're going to use\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, convolutional, pooling, Flatten, Dropout,  Embedding\n",
    "from keras.optimizers import Adam,SGD,RMSprop,Adagrad,Adadelta,Adamax,Nadam\n",
    "# https://github.com/fchollet/keras/issues/3945\n",
    "# http://stackoverflow.com/questions/41651628/negative-dimension-size-caused-by-subtracting-3-from-1-for-conv2d\n",
    "# from keras import backend as K\n",
    "# K.set_image_dim_ordering('th')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# When you execute a code to plot with a simple SHIFT-ENTER, the plot will be shown directly under the code cell\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before pre-processing, X_train size:  (16175, 2304)\n",
      "Before pre-processing, y_train size:  (16175,)\n",
      "Before pre-processing, X_test size:  (3965, 2304)\n",
      "After pre-processing, X_train size:  (16175, 1, 48, 48)\n",
      "After pre-processing, y_train size:  (16175, 3)\n",
      "After pre-processing, X_test size:  (3965, 1, 48, 48)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_data = np.genfromtxt('./train_data.csv',delimiter=',')\n",
    "train_targets = np.genfromtxt('./train_target.csv',delimiter=',')\n",
    "test_data = np.genfromtxt('./test_data.csv',delimiter=',')\n",
    "\n",
    "print('Before pre-processing, X_train size: ', train_data.shape)\n",
    "print('Before pre-processing, y_train size: ', train_targets.shape)\n",
    "print('Before pre-processing, X_test size: ', test_data.shape)\n",
    "\n",
    "train_data = train_data.reshape(-1,1, 48,48)\n",
    "test_data = test_data.reshape(-1,1, 48,48)\n",
    "train_targets = np_utils.to_categorical(train_targets, 3)\n",
    "\n",
    "print('After pre-processing, X_train size: ', train_data.shape)\n",
    "print('After pre-processing, y_train size: ', train_targets.shape)\n",
    "print('After pre-processing, X_test size: ', test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.add(convolutional.Conv2D(\n",
    "    filters=64,\n",
    "    kernel_size=(1, 1),\n",
    "    padding='same',\n",
    "    strides=(1, 1),\n",
    "    input_shape=(1,48,48),\n",
    "    activation='relu',\n",
    "))\n",
    "\n",
    "model.add(convolutional.Conv2D(\n",
    "    filters=64,\n",
    "    kernel_size=(3, 3),\n",
    "    padding='same',\n",
    "    strides=(1, 1),\n",
    "    activation='relu',\n",
    "))\n",
    "\n",
    "model.add(convolutional.Conv2D(\n",
    "    filters=64,\n",
    "    kernel_size=(5, 5),\n",
    "    padding='same',\n",
    "    strides=(1, 1),\n",
    "    activation='relu',\n",
    "))\n",
    "\n",
    "model.add(pooling.MaxPooling2D(\n",
    "    pool_size=(2, 2),\n",
    "    padding='same',\n",
    "))\n",
    "\n",
    "\n",
    "model.add(convolutional.Conv2D(\n",
    "    filters=128,\n",
    "    padding='same',\n",
    "    strides=(1, 1),\n",
    "    kernel_size=(1, 1),\n",
    "    activation='relu',\n",
    "))\n",
    "\n",
    "model.add(convolutional.Conv2D(\n",
    "    filters=128,\n",
    "    padding='same',\n",
    "    strides=(1, 1),\n",
    "    kernel_size=(3, 3),\n",
    "    activation='relu',\n",
    "))\n",
    "\n",
    "model.add(convolutional.Conv2D(\n",
    "    filters=128,\n",
    "    padding='same',\n",
    "    strides=(1, 1),\n",
    "    kernel_size=(5, 5),\n",
    "    activation='relu',\n",
    "))\n",
    "\n",
    "model.add(pooling.MaxPooling2D(\n",
    "    pool_size=(2, 2),\n",
    "    padding='same',\n",
    "))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11322 samples, validate on 4853 samples\n",
      "Epoch 1/100\n",
      "88s - loss: 1.1024 - acc: 0.4454 - val_loss: 1.0141 - val_acc: 0.4978\n",
      "Epoch 2/100\n",
      "88s - loss: 1.0129 - acc: 0.5010 - val_loss: 0.9821 - val_acc: 0.5100\n",
      "Epoch 3/100\n",
      "89s - loss: 0.9694 - acc: 0.5353 - val_loss: 0.9296 - val_acc: 0.5644\n",
      "Epoch 4/100\n",
      "88s - loss: 0.9427 - acc: 0.5519 - val_loss: 0.9218 - val_acc: 0.5574\n",
      "Epoch 5/100\n",
      "88s - loss: 0.9116 - acc: 0.5715 - val_loss: 0.9127 - val_acc: 0.5737\n",
      "Epoch 6/100\n",
      "89s - loss: 0.8997 - acc: 0.5807 - val_loss: 0.8955 - val_acc: 0.5856\n",
      "Epoch 7/100\n",
      "92s - loss: 0.8841 - acc: 0.5904 - val_loss: 0.8822 - val_acc: 0.5902\n",
      "Epoch 8/100\n",
      "87s - loss: 0.8717 - acc: 0.6026 - val_loss: 0.8796 - val_acc: 0.5976\n",
      "Epoch 9/100\n",
      "86s - loss: 0.8549 - acc: 0.6054 - val_loss: 0.8718 - val_acc: 0.5949\n",
      "Epoch 10/100\n",
      "87s - loss: 0.8542 - acc: 0.6103 - val_loss: 0.8753 - val_acc: 0.5988\n",
      "Epoch 11/100\n",
      "85s - loss: 0.8410 - acc: 0.6176 - val_loss: 0.8642 - val_acc: 0.6027\n",
      "Epoch 12/100\n",
      "75s - loss: 0.8348 - acc: 0.6236 - val_loss: 0.8709 - val_acc: 0.6052\n",
      "Epoch 13/100\n",
      "45s - loss: 0.8165 - acc: 0.6343 - val_loss: 0.8600 - val_acc: 0.6023\n",
      "Epoch 14/100\n",
      "45s - loss: 0.8076 - acc: 0.6390 - val_loss: 0.8464 - val_acc: 0.6138\n",
      "Epoch 15/100\n",
      "45s - loss: 0.8076 - acc: 0.6389 - val_loss: 0.8749 - val_acc: 0.6064\n",
      "Epoch 16/100\n",
      "45s - loss: 0.7950 - acc: 0.6423 - val_loss: 0.8620 - val_acc: 0.6066\n",
      "Epoch 17/100\n",
      "46s - loss: 0.7821 - acc: 0.6508 - val_loss: 0.8457 - val_acc: 0.6159\n",
      "Epoch 18/100\n",
      "45s - loss: 0.7717 - acc: 0.6532 - val_loss: 0.8514 - val_acc: 0.6149\n",
      "Epoch 19/100\n",
      "45s - loss: 0.7591 - acc: 0.6607 - val_loss: 0.8818 - val_acc: 0.6073\n",
      "Epoch 20/100\n",
      "45s - loss: 0.7513 - acc: 0.6622 - val_loss: 0.8636 - val_acc: 0.6151\n",
      "Epoch 21/100\n",
      "45s - loss: 0.7412 - acc: 0.6725 - val_loss: 0.8489 - val_acc: 0.6200\n",
      "Epoch 22/100\n",
      "46s - loss: 0.7226 - acc: 0.6772 - val_loss: 0.8980 - val_acc: 0.6178\n",
      "Epoch 23/100\n",
      "45s - loss: 0.7163 - acc: 0.6828 - val_loss: 0.8497 - val_acc: 0.6219\n",
      "Epoch 24/100\n",
      "46s - loss: 0.7063 - acc: 0.6887 - val_loss: 0.8739 - val_acc: 0.6256\n",
      "Epoch 25/100\n",
      "45s - loss: 0.7034 - acc: 0.6895 - val_loss: 0.8765 - val_acc: 0.6077\n",
      "Epoch 26/100\n",
      "47s - loss: 0.6865 - acc: 0.6989 - val_loss: 0.9053 - val_acc: 0.6182\n",
      "Epoch 27/100\n",
      "47s - loss: 0.6737 - acc: 0.7046 - val_loss: 0.9001 - val_acc: 0.6155\n",
      "Epoch 28/100\n",
      "48s - loss: 0.6689 - acc: 0.7078 - val_loss: 0.9050 - val_acc: 0.6103\n",
      "Epoch 29/100\n",
      "53s - loss: 0.6536 - acc: 0.7127 - val_loss: 0.8908 - val_acc: 0.6198\n",
      "Epoch 30/100\n",
      "89s - loss: 0.6472 - acc: 0.7122 - val_loss: 0.9319 - val_acc: 0.6340\n",
      "Epoch 31/100\n",
      "49s - loss: 0.6378 - acc: 0.7265 - val_loss: 0.9051 - val_acc: 0.6283\n",
      "Epoch 32/100\n",
      "49s - loss: 0.6141 - acc: 0.7365 - val_loss: 1.0128 - val_acc: 0.6093\n",
      "Epoch 33/100\n",
      "57s - loss: 0.6091 - acc: 0.7368 - val_loss: 0.9391 - val_acc: 0.6221\n",
      "Epoch 34/100\n",
      "74s - loss: 0.6047 - acc: 0.7357 - val_loss: 0.9973 - val_acc: 0.6174\n",
      "Epoch 35/100\n",
      "90s - loss: 0.5960 - acc: 0.7381 - val_loss: 0.9579 - val_acc: 0.6246\n",
      "Epoch 36/100\n"
     ]
    }
   ],
   "source": [
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "sgd = SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "rms_prop = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "adagrad = Adagrad(lr=0.01, epsilon=1e-08, decay=0.0)\n",
    "adadelta = Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0)\n",
    "adamax = Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "nadam = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer = adam,\n",
    "#     optimizer = 'adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# model.fit(train_data, train_targets,epochs=10,batch_size=32,verbose=2)\n",
    "model.fit(train_data, train_targets, validation_split=0.30, epochs=100, batch_size=64, verbose=2)\n",
    "\n",
    "# loss, accuracy = model.evaluate(test_data, test_target, verbose=2)\n",
    "# print('test loss:', loss)\n",
    "# print('test accuracy', accuracy)\n",
    "\n",
    "\n",
    "# output = model.predict(X_test[temp[0]].reshape(-1,1, 28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the prediction\n",
    "predictions = model.predict(test_data)\n",
    "\n",
    "output = predictions.argmax(axis=1)\n",
    "\n",
    "f = open('./result3.csv','w')\n",
    "f.write('Id,Category\\n') #Give your csv text here.\n",
    "for i in range(0, test_data.shape[0]):\n",
    "    f.write(str(i) + ',' + str(output[i]) + '\\n')\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
