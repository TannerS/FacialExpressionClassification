{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# The modules we're going to use\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, convolutional, pooling, Flatten, Dropout\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the training and target data from the data set (which is csv)\n",
    "train_data = np.genfromtxt('./train_data.csv',delimiter=',')\n",
    "train_target = np.genfromtxt('./train_target.csv', delimiter=',')\n",
    "# load the test data also\n",
    "test_data = np.genfromtxt('./test_data.csv',delimiter=',')\n",
    "# here we split the training data to get some data for cross validation\n",
    "train_data, val_data, train_target, val_target = train_test_split(train_data, (train_target[:, np.newaxis]), test_size=0.3, random_state=42)\n",
    "# display dataset shapes before processing\n",
    "print('Before pre-processing, X_train size: ', train_data.shape)\n",
    "print('Before pre-processing, y_train size: ', train_target.shape)\n",
    "print('Before pre-processing, X_test size: ', test_data.shape)\n",
    "print('Before pre-processing, X_val size: ', val_data.shape)\n",
    "print('Before pre-processing, y_val size: ', val_target.shape)\n",
    "# reshape the data to match the pixles of the image (-1, = stays same, 1 = 1 channel, 48 & 48means 48x48 image)\n",
    "train_data = train_data.reshape(-1,1, 48,48)\n",
    "test_data = test_data.reshape(-1,1, 48,48)\n",
    "val_data = val_data.reshape(-1,1, 48,48)\n",
    "# break down the targets into forms of 0,1, or 2\n",
    "train_target = np_utils.to_categorical(train_target, 3)\n",
    "val_target = np_utils.to_categorical(val_target, 3)\n",
    "# display the new shapes\n",
    "print('After pre-processing, X_train size: ', train_data.shape)\n",
    "print('After pre-processing, y_train size: ', train_target.shape)\n",
    "print('After pre-processing, X_test size: ', test_data.shape)\n",
    "print('After pre-processing, X_val size: ', val_data.shape)\n",
    "print('After pre-processing, y_val size: ', val_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a new model\n",
    "model = Sequential()\n",
    "# create a convolutional layer for 2 dimensions\n",
    "# this one includes the input size ofr first layer\n",
    "model.add(convolutional.Conv2D(\n",
    "    filters=32,\n",
    "    kernel_size=(2, 2),\n",
    "    padding='same',\n",
    "    strides=(1, 1),\n",
    "    input_shape=train_data.shape[1:],\n",
    "    activation='relu'\n",
    "))\n",
    "# create a convolutional layer for 2 dimensions\n",
    "model.add(convolutional.Conv2D(\n",
    "    filters=32,\n",
    "    kernel_size=(2, 2),\n",
    "    padding='same',\n",
    "    strides=(1, 1),\n",
    "    activation='relu'\n",
    "))\n",
    "# create a convolutional layer for 2 dimensions\n",
    "model.add(convolutional.Conv2D(\n",
    "    filters=32,\n",
    "    kernel_size=(2, 2),\n",
    "    padding='same',\n",
    "    strides=(1, 1),\n",
    "    activation='relu'\n",
    "))\n",
    "# create a max pooling layer for 2 dimensions\n",
    "model.add(pooling.MaxPooling2D(\n",
    "    pool_size=(2, 2),\n",
    "    padding='same',\n",
    "))\n",
    "# create a convolutional layer for 2 dimensions\n",
    "model.add(convolutional.Conv2D(\n",
    "    filters=64,\n",
    "    kernel_size=(2, 2),\n",
    "    padding='same',\n",
    "    strides=(1, 1),\n",
    "    activation='relu'\n",
    "))\n",
    "# create a convolutional layer for 2 dimensions\n",
    "model.add(convolutional.Conv2D(\n",
    "    filters=64,\n",
    "    kernel_size=(2, 2),\n",
    "    padding='same',\n",
    "    strides=(1, 1),\n",
    "    activation='relu'\n",
    "))\n",
    "# create a convolutional layer for 2 dimensions\n",
    "model.add(convolutional.Conv2D(\n",
    "    filters=64,\n",
    "    kernel_size=(2, 2),\n",
    "    padding='same',\n",
    "    strides=(1, 1),\n",
    "    activation='relu'\n",
    "))\n",
    "# create a max pooling layer for 2 dimensions\n",
    "model.add(pooling.MaxPooling2D(\n",
    "    pool_size=(2, 2),\n",
    "    padding='same',\n",
    "))\n",
    "# create a convolutional layer for 2 dimensions\n",
    "model.add(convolutional.Conv2D(\n",
    "    filters=128,\n",
    "    kernel_size=(2, 2),\n",
    "    padding='same',\n",
    "    strides=(1, 1),\n",
    "    activation='relu'\n",
    "))\n",
    "# create a convolutional layer for 2 dimensions\n",
    "model.add(convolutional.Conv2D(\n",
    "    filters=128,\n",
    "    kernel_size=(2, 2),\n",
    "    padding='same',\n",
    "    strides=(1, 1),\n",
    "    activation='relu'\n",
    "))\n",
    "# create a convolutional layer for 2 dimensions\n",
    "model.add(convolutional.Conv2D(\n",
    "    filters=128,\n",
    "    kernel_size=(2, 2),\n",
    "    padding='same',\n",
    "    strides=(1, 1),\n",
    "    activation='relu'\n",
    "))\n",
    "# create a max pooling layer for 2 dimensions\n",
    "model.add(pooling.MaxPooling2D(\n",
    "    pool_size=(2, 2),\n",
    "    padding='same',\n",
    "))\n",
    "# flatten the model's layer into a deep neuron which in turn, will be part of the fully connected feedforward network\n",
    "model.add(Flatten())\n",
    "# create a single deep layer with a depth of 1024 for the output space\n",
    "model.add(Dense(1024, dropout=0.5))\n",
    "# choosing this as the activation type\n",
    "model.add(Activation('relu'))\n",
    "# last layer is the softmax layer to et the probability of each class\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# choose adam as the optimizer\n",
    "# the optmizer chooses the adaptive learnign rates which are used for the Stochastic gradient descent\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "# compile (configure) the model's learning process\n",
    "model.compile(\n",
    "    # choose adam as optimizer\n",
    "    optimizer = adam,\n",
    "    # uses cross entroyphy for loss function\n",
    "    loss='categorical_crossentropy',\n",
    "    # A metric function is similar to an loss function, except that the results from evaluating a metric are not used when training the model.\n",
    "    # https://keras.io/metrics/\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "# fit the model with the data to train ig, and data to vlaidate it,\n",
    "# also define the epochs and the batch size for each epochs\n",
    "model.fit(train_data, train_target, validation_data=(val_data, val_target), epochs=50, batch_size=128, verbose=2)\n",
    "# model.fit(train_data, train_target, validation_split=0.3, validation_data=val_data, epochs=50, batch_size=128, verbose=2)\n",
    "# get loss amount and accuracy of the validation set\n",
    "loss, accuracy = model.evaluate(val_data, val_target, verbose=2)\n",
    "print('test loss:', loss)\n",
    "print('test accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Show the image of one testing example\n",
    "# get random number within range of the test data\n",
    "temp = np.random.randint(test_data.shape[0], size=1)\n",
    "# Get its prediction\n",
    "output = model.predict(test_data[temp[0]].reshape(-1,1, 48, 48))\n",
    "# output prediction\n",
    "print(output)\n",
    "# create new figure using matplotlib\n",
    "plt.figure()\n",
    "# display and plot image\n",
    "plt.xticks(np.arange(output.shape[1]))\n",
    "plt.plot(np.arange(output.shape[1]), output.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# get predictions\n",
    "predictions = model.predict(test_data)\n",
    "# The maximum value along a given axis.\n",
    "# https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.argmax.html\n",
    "output = predictions.argmax(axis=1)\n",
    "# open new file of my choosing\n",
    "f = open('./tanner_summers_hw2_results.csv','w')\n",
    "# Write category header\n",
    "f.write('Id,Category\\n')\n",
    "# loop data and print to file the iteration and it's target for that iteration at i\n",
    "for i in range(0, test_data.shape[0]):\n",
    "    # write data to file\n",
    "    f.write(str(i) + ',' + str(output[i]) + '\\n')\n",
    "# close file\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
